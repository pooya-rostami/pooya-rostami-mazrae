"use strict";(self.webpackChunkpooya_rostami_mazrae=self.webpackChunkpooya_rostami_mazrae||[]).push([[749],{1895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"Hybrid-Linker-Automated-Recover-of-Issue-Commit-Links","metadata":{"permalink":"/blog/Hybrid-Linker-Automated-Recover-of-Issue-Commit-Links","source":"@site/blog/3_Hybrid-Linker_Automated_Recovery_of_Issue-Commit_Links.md","title":"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","description":"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","date":"2025-04-21T12:59:44.000Z","tags":[{"inline":false,"label":"Research","permalink":"/blog/tags/research","description":"Scientific research i have participated in"},{"inline":false,"label":"SoftwareEngineering","permalink":"/blog/tags/SoftwareEngineering","description":"related to Software Engineering"}],"readingTime":6.055,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"Hybrid-Linker-Automated-Recover-of-Issue-Commit-Links","description":"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","title":"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","tags":["Research","SoftwareEngineering"]},"unlisted":false,"nextItem":{"title":"Preliminary Study of GitHub Actions Workflow Changes","permalink":"/blog/Preliminary-Study-of-GitHub-Actions-Workflow-Changes"}},"content":"## Introduction\\n\\nIssue reports document discussions around required changes in issue-tracking systems, while commits contain the actual code changes in version control systems. Recovering links between issues and commits facilitates many software evolution tasks such as bug localization, defect prediction, software quality measurement, and documentation.\\n\\nA previous study on over half a million GitHub issues showed that only about 42.2% of issues are manually linked by developers to their related commits. Automating the linking of commit-issue pairs can significantly improve software maintenance tasks. However, current state-of-the-art approaches suffer from low precision, leading to unreliable results, and perform poorly when there\'s a lack of textual information in commits or issues.\\n\\nThis article presents **Hybrid-Linker**, an enhanced approach that overcomes these limitations by exploiting both textual and non-textual data channels:\\n\\n1. A **non-textual-based component** that operates on automatically recorded metadata of commit-issue pairs\\n2. A **textual-based component** that analyzes the textual content of commits and issues\\n\\nBy combining results from these two classifiers, Hybrid-Linker makes the final prediction, with one component filling gaps when the other falls short.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Motivating Example\\n\\nLet\'s look at a practical example to understand the challenge. Consider an issue from the Apache Flink project:\\n\\n![Example of an issue and its related commit](../static/blog_posts/Automated_Recovery_of_Issue-Commit_Links/Example%20of%20an%20issue%20and%20its%20related%20commit.jpg)\\n\\nThe issue titled \\"copy method logicalaggregate not copying indicator value properly\\" lacks compelling similarity between its description text, release notes, and the corresponding commit message. Due to this lack of textual similarity, previous approaches like FRLink and DeepLink struggle to discover the connection between this issue and its commit.\\n\\nThis example highlights the need to extract knowledge from both textual and non-textual information channels of issues and commits to improve link recovery.\\n\\n## Approach\\n\\nHybrid-Linker follows five main steps:\\n\\n### 1. Data Crawling\\n\\nThe approach starts with collecting data from issue tracking systems and version control systems, including code diff data which requires special handling due to its large volume.\\n\\n### 2. Data Preparation\\n\\nTwo essential preparation steps are performed:\\n\\n#### Link Generation\\n- **True Links**: Issue-commit pairs already linked by developers\\n- **False Links**: Generated by pairing commits with issues other than the ones they\'re already linked to\\n- To address dataset imbalance, the approach follows criteria from previous works (comparing submission dates) and applies data balancing techniques\\n\\n#### Textual Data Preprocessing\\n- Standard NLP techniques are applied to natural language text: tokenizing, removing stop words, and stemming\\n- For code diff data, only identifiers (method and variable names) are extracted as they carry the most valuable information about changes\\n\\n### 3. Feature Engineering\\n\\n#### Textual Feature Engineering\\n- The TF-IDF technique is applied to natural language text data of commits and issues and the code diff textual data separately\\n- Three vectors are generated and concatenated to create one textual feature vector per data point\\n\\n#### Non-textual Feature Engineering\\n- Highly correlated columns are reduced to avoid redundancy\\n- Categorical data is transformed and optimized to avoid sparse matrices\\n- Temporal data (dates) is preserved as important features\\n\\n### 4. Model Training\\n\\nTwo classifier components are trained:\\n\\n![Overview of the proposed approach](../static/blog_posts/Automated_Recovery_of_Issue-Commit_Links/Overview%20of%20the%20proposed%20approach.jpg)\\n\\n#### Textual Classifier Model\\nMultiple classification models are trained and evaluated:\\n- Decision Tree\\n- Gradient Boosting\\n- Logistic Regression\\n- Stochastic Gradient Descent\\n\\n#### Non-textual Classifier Model\\nBoth single and ensemble models are trained:\\n- Simple methods: Gradient Boosting, Naive Bayes, Generalized Linear, Random Forest, XGBoost\\n- Ensemble methods: Various combinations of the above models\\n\\n### 5. Linear Accumulator Hyper-tuning\\n\\nThe final prediction combines results from both classifiers using a linear accumulator function:\\n\\n```\\nP_final = \u03b1 \xd7 P_non-textual + (1 \u2212 \u03b1) \xd7 P_textual\\n```\\n\\nThe \u03b1 hyperparameter is tuned for each project to produce optimal results.\\n\\n## Evaluation\\n\\n### Research Questions\\n\\n1. **RQ1**: How effective is Hybrid-Linker compared to state-of-the-art approaches?\\n2. **RQ2**: How to combine the two components to achieve the best outcome?\\n3. **RQ3**: What is the effect of each component on the outcome?\\n\\n### Dataset and Metrics\\n\\nThe study used 12 Apache projects selected based on:\\n- Having repositories with more than 500 stars\\n- Having diverse numbers of issues\\n\\n![Selected projects information](../static/blog_posts/Automated_Recovery_of_Issue-Commit_Links/Selected%20projects%20information.jpg)\\n\\nEvaluation metrics included Precision, Recall, and F-measure, using five-fold cross-validation.\\n\\n### Results\\n\\n#### RQ1: Effectiveness Compared to State-of-the-Art\\n\\nFor the textual data, we found out that `Gradient Boosting` is the best option and for non-textual data we went with the ensemble\\nof `Gradient Boosting` and `XGBoost`.\\n\\nPerformance of the approach across projects:\\n\\n| Approach      | Recall | Precision | F-measure |\\n|---------------|--------|-----------|-----------|\\n| Hybrid-Linker | 90.14% | 87.78%    | 88.88%    |\\n| DeepLink      | 60.09% | 68.81%    | 62.87%    |\\n| FRLink        | 96.86% | 53.61%    | 67.67%    |\\n\\nHybrid-Linker outperformed:\\n- FRLink by 31.3% regarding F-measure\\n- DeepLink by 41.3% regarding F-measure\\n\\nAdditionally, Hybrid-Linker was significantly more efficient, requiring far less training time than DeepLink. \\nFor instance, training on the Airflow project took only 25 minutes for Hybrid-Linker versus 7 hours for DeepLink.\\n\\n#### RQ2: Optimal Component Combination\\n\\nEach project required a different value of \u03b1 for optimal results:\\n\\n- Average \u03b1 value across all projects: 0.66\\n- Most projects had \u03b1 above 0.5, indicating non-textual components played a more important role\\n- Projects with extremes: Calcite (\u03b1 = 0.95) and Ambari (\u03b1 = 0.45)\\n\\n#### RQ3: Component Effect Analysis\\n\\n| Model       | Recall | Precision | F-measure | Std Dev |\\n|-------------|--------|-----------|-----------|---------|\\n| Hybrid      | 90.14% | 87.78%    | 88.88%    | 3.00    |\\n| Textual     | 80.83% | 80.92%    | 80.82%    | 5.83    |\\n| Non-textual | 85.57% | 91.63%    | 88.36%    | 3.10    |\\n\\nKey findings:\\n- The textual model had the lowest performance and highest standard deviation\\n- The non-textual model outperformed the hybrid model in precision\\n- The hybrid model achieved the highest recall and F-measure scores with the lowest standard deviation\\n\\n### Example Success Case\\n\\nAn example shows where the model successfully recovered a true link when textual information was insufficient:\\n\\n| Issue Information                                                             | Commit Information                                                                                       |\\n|-------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\\n| Created date: 2014-12-08                                                      | Author time date: 2014-12-10                                                                             |\\n| Summary: \\"copy method logical aggregate not copying indicator value properly\\" | Message: \\"[ calcite-511 ] copy method logical aggregate not copying indicator value properly fixes # 26\\" |\\n| Bug: 1, feature: 0, task: 0                                                   | DiffCode: \\"logical aggregate .java...\\"                                                                   |\\n\\nDespite few textual similarities, the non-textual component compensated for this shortcoming and predicted the correct connection.\\n\\n## Conclusion\\n\\nHybrid-Linker leverages both textual and non-textual information to recover links between issues and commits. By tuning the importance of each information channel (via parameter \u03b1), it achieves superior performance compared to existing approaches, especially in cases with limited textual information.\\n\\nThe approach outperforms competing methods with:\\n- Higher accuracy (F-measure improvement of 31-41%)\\n- Shorter training time\\n- Better adaptability across different projects\\n\\nThe research demonstrates that non-textual information is often critical to accurate link prediction, particularly when textual similarities are minimal.\\n\\nFuture work will focus on identifying new features from different bug tracking and version control systems, and investigating other classifier architectures.\\n\\nTo check the paper click on following [link](https://ieeexplore.ieee.org/abstract/document/9609165) or download directly from following [link](../static/papers/ICSME_2021_AutomatedRecoveryOfIssueCommitLinks.pdf).\\n\\nCitation:\\n```\\n@inproceedings{mazrae2021automated,\\n  title={Automated recovery of issue-commit links leveraging both textual and non-textual data},\\n  author={Mazrae, Pooya Rostami and Izadi, Maliheh and Heydarnoori, Abbas},\\n  booktitle={2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},\\n  pages={263--273},\\n  year={2021},\\n  organization={IEEE}\\n}\\n```"},{"id":"Preliminary-Study-of-GitHub-Actions-Workflow-Changes","metadata":{"permalink":"/blog/Preliminary-Study-of-GitHub-Actions-Workflow-Changes","source":"@site/blog/2_priliminary_study_of_gha_workflows.md","title":"Preliminary Study of GitHub Actions Workflow Changes","description":"Preliminary Study of GitHub Actions Workflow Changes","date":"2025-04-15T09:59:19.000Z","tags":[{"inline":false,"label":"Research","permalink":"/blog/tags/research","description":"Scientific research i have participated in"},{"inline":false,"label":"SoftwareEngineering","permalink":"/blog/tags/SoftwareEngineering","description":"related to Software Engineering"}],"readingTime":5.42,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"Preliminary-Study-of-GitHub-Actions-Workflow-Changes","description":"Preliminary Study of GitHub Actions Workflow Changes","title":"Preliminary Study of GitHub Actions Workflow Changes","tags":["Research","SoftwareEngineering"]},"unlisted":false,"prevItem":{"title":"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","permalink":"/blog/Hybrid-Linker-Automated-Recover-of-Issue-Commit-Links"},"nextItem":{"title":"Bot Detection in GitHub Repositories","permalink":"/blog/Bot-Detection-in-GitHub-Repositories"}},"content":"## Introduction\\n\\nContinuous Integration and Development (CI/CD) practices have become essential in modern software development, helping developers release high-quality software more efficiently. GitHub Actions (GHA) was introduced in November 2019 as GitHub\'s native CI/CD solution and quickly gained popularity, replacing other tools like Travis CI within just 18 months.\\n\\nThis article examines how GitHub Actions workflows evolve over time, based on a study of 22,733 GitHub repositories containing 4 million weekly snapshots of workflow files from November 2019 to September 2022. The research analyzes both coarse-grained changes (adding, modifying, renaming, or removing workflow files) and fine-grained changes (examining line-level modifications within these files).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background on GitHub Actions\\n\\nGitHub Actions allows developers to automate various aspects of the software development lifecycle. Workflows are configured using YAML files stored in the `.github/workflows/` directory of a repository. These workflows can automate tasks like testing, building, quality checking, dependency management, and security scanning.\\n\\nAs with any software component, workflow files evolve over time to better serve their purposes. Understanding how workflows change can help improve CI/CD practices and assist developers in maintaining their workflows more effectively.\\n\\n## Example of Workflow Changes\\n\\nConsider a workflow file for a Java project that evolves over time. Changes might include:\\n\\n1. Updating the triggers that activate the workflow\\n2. Adding new steps or modifying existing ones\\n3. Updating versions of actions being used\\n4. Changing configuration parameters\\n5. Renaming steps for clarity\\n\\n![workflow file visual diff](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/yaml_diff_example.jpg)\\n\\nThese changes demonstrate how workflows evolve to accommodate new requirements, fix issues, or adopt best practices.\\n\\n## Research Methodology\\n\\nThe study analyzed 22,733 GitHub repositories with at least 100 stars and 100 commits, created before 2022 and still active in 2022. For each repository, weekly snapshots of workflow files were collected between November 2019 and September 2022, resulting in 4,127,760 workflow file snapshots across 148 time points.\\n\\n![Evolution of repositories and workflows over time](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Evolution%20of%20repositories%20and%20workflows%20over%20time.jpg)\\n\\nThe figure shows continuous growth in both the number of repositories using GitHub Actions and the number of workflow files over the study period, with a notable increase in November-December 2020 when Travis CI imposed restrictions on its free plan.\\n\\n## When Do Repositories Start Using GitHub Actions?\\n\\nThe study distinguished between repositories that existed before GHA was introduced and those created after. For repositories created after GHA\'s release:\\n- 50% adopted GHA within 3 months of creation\\n- 75% adopted GHA within 10 months\\n\\nFor repositories that existed before GHA\'s release:\\n- Only 15% adopted GHA within 3 months of its release\\n- 50% adopted GHA within 10 months\\n- 75% adopted GHA within 21 months\\n\\n![Time to adopt GitHub Actions based on repository creation date](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Time%20to%20adopt%20GitHub%20Actions%20based%20on%20repository%20creation%20date.jpg)\\n\\nThis suggests that repositories created after GHA\'s introduction adopt it quickly, while older repositories take longer to migrate, likely due to existing CI/CD setups and migration challenges.\\n\\n## Types of Coarse-Grained Changes in Workflows\\n\\nThe study identified four types of coarse-grained changes:\\n1. **Addition**: The first appearance of a workflow file\\n2. **Modification**: Changes to the content of an existing workflow file\\n3. **Removal**: Deletion of a workflow file\\n4. **Renaming**: Changing the name of a workflow file\\n\\nThe most frequent change type is modification (73% of all changes), followed by addition (22.8%), removal (3.9%), and renaming (0.1%).\\n\\n![Proportion of repositories exhibiting different types of changes over time](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Proportion%20of%20repositories%20exhibiting%20different%20types%20of%20changes%20over%20time.jpg)\\n\\nOn average each week:\\n- 7.2% of repositories modify a workflow file\\n- 1.4% add a new workflow file\\n- 0.5% remove a workflow file\\n\\n## When Do Coarse-Grained Changes Occur?\\n\\nChanges to workflow files are more frequent during the initial weeks after adopting GitHub Actions. In the first week after adoption, more than 15% of repositories make changes to their workflows, but this decreases to around 6% after six weeks.\\n\\n![Proportion of repositories exhibiting changes based on time since GHA adoption](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Proportion%20of%20repositories%20exhibiting%20changes%20based%20on%20time%20since%20GHA%20adoption.jpg)\\n\\nThis pattern follows Lehman\'s software evolution laws of continuing change and continuing growth. While the frequency of changes decreases over time, repositories continue to modify workflows throughout their lifecycle.\\n\\n## Fine-Grained Changes to Workflow Files\\n\\nThe study also analyzed line-based changes within workflow files, finding that the vast majority (87.8%) of changes involve lines of code rather than blank lines or comments. In fact, 97.6% of all modifications include at least some code changes.\\n\\nLooking at the nature of line-based changes:\\n- 78.7% of changes include modifying existing lines\\n- 50.1% include adding new lines\\n- 26.2% include removing lines\\n\\n![Distribution of workflow changes by line operation type](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Distribution%20of%20workflow%20changes%20by%20line%20operation%20type.jpg)\\n\\nOnly 3.45% of changes involve exclusively removing lines, while 40.11% involve only modifying lines.\\n\\n## When Do Fine-Grained Changes Occur?\\n\\nOn average, about 10% of lines in workflow files are touched (added, removed, or modified) when changes occur. This proportion is higher during the first few weeks after adopting GitHub Actions.\\n\\n![Proportion of lines touched during workflow changes over time](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Proportion%20of%20lines%20touched%20during%20workflow%20changes%20over%20time.jpg)\\n\\nFor line additions, the number decreases over time:\\n- Average of 7.4 lines added during the first six weeks\\n- Average of 5.6 lines added during the remainder of the first year\\n\\n![Number of lines added during workflow changes over time](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Number%20of%20lines%20added%20during%20workflow%20changes%20over%20time.jpg)\\n\\nFor line modifications, the pattern remains relatively stable:\\n- Average of 2.8 lines modified during the first six weeks\\n- Average of 3.1 lines modified during the remainder of the first year\\n\\n![Number of lines modified during workflow changes over time](../static/blog_posts/priliminary_study_of_github_actions_workflow_changes/Number%20of%20lines%20modified%20during%20workflow%20changes%20over%20time.jpg)\\n\\nThese findings suggest that workflow files are continuously evolving, with more substantial changes occurring early in their lifecycle, followed by regular but smaller maintenance changes.\\n\\n## Conclusion\\n\\nThis study reveals several key insights about GitHub Actions workflow evolution:\\n\\n1. **Adoption patterns**: New repositories quickly adopt GitHub Actions, while existing repositories take longer to migrate from other CI/CD systems.\\n\\n2. **Change frequency**: Workflows undergo frequent changes, particularly during their early lifecycle, with modifications being the most common type of change.\\n\\n3. **Evolution patterns**: Workflow files follow Lehman\'s software evolution laws of continuing change and continuing growth, suggesting they evolve similarly to other software artifacts.\\n\\n4. **Line-level changes**: Most changes involve modifying or adding lines of code, with an average of 3.1 lines modified and 5.8 lines added per workflow change.\\n\\nThese findings provide valuable insights for developers and researchers interested in CI/CD practices and workflow automation. The study lays groundwork for future research into workflow quality, complexity, and maintenance practices.\\n\\nTo check the paper click on following [link](https://ceur-ws.org/Vol-3483/paper8.pdf) or download directly from following [link](../static/papers/Sattose_2023_A%20Preliminary%20Study%20of%20GitHub%20Actions%20Workflow%20Changes.pdf).\\n\\nCitation:\\n```\\n@article{mazrae2023preliminary,\\n  title={A preliminary study of github actions workflow changes},\\n  author={Mazrae, P Rostami and Decan, Alexandre and Mens, Tom and Wessel, Mairieli},\\n  year={2023},\\n  publisher={Sl: CEUR}\\n}\\n```"},{"id":"Bot-Detection-in-GitHub-Repositories","metadata":{"permalink":"/blog/Bot-Detection-in-GitHub-Repositories","source":"@site/blog/1_Bot_Detection_in_GitHub_Repositories.md","title":"Bot Detection in GitHub Repositories","description":"Bot Detection in GitHub Repositories","date":"2025-04-15T08:33:06.000Z","tags":[{"inline":false,"label":"Research","permalink":"/blog/tags/research","description":"Scientific research i have participated in"},{"inline":false,"label":"SoftwareEngineering","permalink":"/blog/tags/SoftwareEngineering","description":"related to Software Engineering"}],"readingTime":4.635,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"Bot-Detection-in-GitHub-Repositories","description":"Bot Detection in GitHub Repositories","title":"Bot Detection in GitHub Repositories","tags":["Research","SoftwareEngineering"]},"unlisted":false,"prevItem":{"title":"Preliminary Study of GitHub Actions Workflow Changes","permalink":"/blog/Preliminary-Study-of-GitHub-Actions-Workflow-Changes"}},"content":"## Introduction\\n\\nIn today\'s collaborative software development landscape, GitHub and similar platforms have revolutionized how developers work together. \\nHowever, with this increased collaboration comes a significant workload of repetitive, error-prone, and time-intensive tasks such as:\\n\\n- Conducting quality checks\\n- Running automated tests\\n- Reviewing code\\n- Merging pull requests\\n- Building and deploying software\\n\\nTo manage this growing complexity, development teams increasingly rely on bots \u2014machine accounts that operate with minimal human intervention\u2014 to automate these tasks. \\nAs [Erlenhov et al. (2019)](https://ieeexplore.ieee.org/abstract/document/8823643) noted, bots have become essential tools for facing the ever-increasing complexity in software development.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Challenge of Bot Identification\\n\\nDespite their prevalence, many bot accounts aren\'t easily identifiable. Consider these examples:\\n\\n- The **highfive** account greets and assigns issues to contributors in the servo/servo repository but isn\'t explicitly labeled as a bot\\n- The **bors-diem** bot manages PR merging in the diem/diem package without \\"bot\\" in its name\\n- Many bots have ordinary GitHub usernames with no indication in their profile description\\n\\nThis identification challenge creates problems for:\\n\\n1. **Researchers** trying to analyze human collaboration patterns in the presence of bots\\n2. **Project managers** attempting to assess the impact of bots on development\\n3. **Organizations** wanting to properly credit human contributors\\n4. **Analysts** calculating \\"bus factors\\" and identifying key project contributors\\n\\n## The Integration Project\\n\\nThis project aims to solve this problem by integrating [BoDeGHa](https://www.sciencedirect.com/science/article/abs/pii/S016412122100008X), \\na machine learning-based bot detection tool, with the [GrimoireLab](https://chaoss.github.io/grimoirelab/) software development analytics platform. \\nGrimoireLab already provides excellent capabilities for:\\n\\n- Data retrieval through Perceval and Graal\\n- Data enrichment via HatStall and SortingHat\\n- Data visualization using Kidash and KiBiter\\n\\nAdding BoDeGHa\'s trained ML classifier as a data enrichment component creates a complete end-to-end pipeline for analyzing bot vs. human contributions.\\n\\n## How The Pipeline Works\\n\\n![Pipeline Diagram](../static/blog_posts/bot_detection_in_GitHub_repositories/Pipeline.jpg)\\n\\nThe integration works through the following steps:\\n\\n1. **Data Collection**: Perceval queries the GitHub API with repository details and authentication token to extract issues, PRs, and their associated comments\\n\\n2. **Data Processing**: The system extracts relevant fields:\\n    - Issue/PR number\\n    - Comment text\\n    - Creation timestamp\\n    - Author names\\n\\n3. **Bot Classification**: BoDeGHa\'s machine learning model analyzes comment patterns to identify bots:\\n    - Repetitive comment structures\\n    - Timing patterns\\n    - Content signatures characteristic of automated responses\\n\\n4. **Results Storage**: Contributor names and their classification (bot or human) are stored in a structured format\\n\\n5. **Visualization**: Kibana dashboards transform the data into visual insights about contributor types and activities\\n\\n## Testing the Integration\\n\\nTo demonstrate this pipeline, we analyzed four popular Cargo package repositories:\\n\\n![Contributor Data](../static/blog_posts/bot_detection_in_GitHub_repositories/Contributor_Data.jpg)\\n\\n- **paritytech/substrate**\\n- **servo/servo**\\n- **diem/diem**\\n- **tokio-rs/tokio**\\n\\nThese repositories were selected due to their size and popularity, making them likely candidates for bot usage in their development processes.\\n\\n### Results\\n\\nThe analysis period covered December 2021 through January 2022, examining contributors who actively posted comments during this timeframe.\\n\\n![Contributor Analysis Results](../static/blog_posts/bot_detection_in_GitHub_repositories/Contributor_Analysis_Results.jpg)\\n\\nOur findings revealed significant bot presence:\\n- **paritytech/substrate**: 6 out of 37 contributors (16.2%) were bots\\n- **diem/diem**: 8 out of 24 contributors (33.3%) were bots\\n- **servo/servo**: 2 out of 6 contributors (33.3%) were bots\\n- **tokio-rs/tokio**: All contributors were human\\n\\nThe relatively high proportion of bot contributors (approaching one-third in some repositories) highlights the importance of distinguishing between human and automated activity when analyzing project contributions.\\n\\n## Potential Extensions and Applications\\n\\nThis integration opens up several valuable possibilities:\\n\\n### Advanced Visualizations\\n\\n- **Contribution Volume Analysis**: Charts showing the proportion of comments/PRs handled by bots vs. humans\\n- **Bot Activity Patterns**: Visualizations of when and how bots are most active in repositories\\n- **Cross-Repository Bot Usage**: Analysis of which bots are used across multiple projects\\n\\n### Enhanced Classification\\n\\n- **User Interface for Corrections**: A mechanism for users to rectify occasional misclassifications\\n- **Classification Confidence**: Indicators showing the ML model\'s confidence in each bot/human prediction\\n- **Custom Training**: Ability to train the model on specific repositories for improved accuracy\\n\\n### Integration with Existing Tools\\n\\n- **SortingHat Connection**: Linking to GrimoireLab\'s contributor management component to maintain bot flags\\n- **Kibiter Display**: Enhanced data visualization showing bot status in contributor profiles\\n\\n## Practical Applications\\n\\nThis bot detection capability enables several practical applications:\\n\\n1. **True Bus Factor Calculation**: Accurately assess project risk by counting only human contributors\\n2. **Human Contribution Metrics**: Generate reports that highlight the work done by human team members\\n3. **Bot Performance Analysis**: Evaluate how effectively different bots are serving their intended purposes\\n4. **Community Growth Tracking**: Monitor genuine human community growth around open-source projects\\n\\n## Conclusion\\n\\nAs bots become more prevalent in collaborative software development, the ability to distinguish between human and automated contributions grows increasingly important. The integration of BoDeGHa\'s machine learning classifier into the GrimoireLab pipeline provides a powerful tool for researchers and practitioners seeking to understand the complex dynamics of modern software development teams.\\n\\nBy accurately identifying bot accounts based on their commenting activities in issues and pull requests, this integration enables more nuanced and accurate socio-technical analysis of software repositories. For organizations and project maintainers, it offers a clearer picture of human contributions, facilitating proper recognition and credit allocation.\\n\\n---\\n\\n*This is an expanded summary of the research paper \\"Bot Detection in GitHub Repositories\\" by Natarajan Chidambaram and Pooya Rostami Mazrae, presented at the 19th International Conference on Mining Software Repositories (MSR \'22).*\\n\\nTo check the paper click on following [link](https://dl.acm.org/doi/10.1145/3524842.3528520) or download directly from following [link](../static/papers/MSR_hackathon_2022_BotDetectionInGitHubRepository.pdf) \\nand for the implementation you can check the [GitHub repository](https://github.com/pooya-rostami/Hackathon-21).\\n\\nCitation:\\n```\\n@inproceedings{chidambaram2022bot,\\n  title={Bot detection in github repositories},\\n  author={Chidambaram, Natarajan and Mazrae, Pooya Rostami},\\n  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},\\n  pages={726--728},\\n  year={2022}\\n}\\n```"}]}}')}}]);